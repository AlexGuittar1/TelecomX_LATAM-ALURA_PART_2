{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom X — Preprocesamiento y Modelado de Churn\n\nNotebook listo para ejecutar: carga el CSV limpio en `data/TelecomX_cleaned.csv`. Este notebook realiza encoding, balanceo opcional (SMOTE), escalado, entrenamiento y evaluación de Regresión Logística y Random Forest, además de análisis de importancia de variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Instalar dependencias (si hace falta)\nDescomenta y ejecuta la celda si necesitas instalar librerías:\n\n```python\n# !pip install scikit-learn imbalanced-learn matplotlib pandas numpy joblib\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\nimport os, pandas as pd, numpy as np\nfrom pathlib import Path\nprint(\"Python and basic libs available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar datos\nColoca `TelecomX_cleaned.csv` en la carpeta `data/` del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\npossible = [\n    \"data/TelecomX_cleaned.csv\",\n    \"/mnt/data/telecomx_churn_project/data/TelecomX_cleaned.csv\",\n    \"TelecomX_cleaned.csv\"\n]\ncsv_path = None\nfor p in possible:\n    if os.path.exists(p):\n        csv_path = p\n        break\nif csv_path is None:\n    raise FileNotFoundError(\"No se encontró data/TelecomX_cleaned.csv. Por favor sube el CSV limpio y re-ejecuta.\")\ndf = pd.read_csv(csv_path)\nprint(\"Shape:\", df.shape)\ndf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Eliminar columnas irrelevantes (IDs)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\nfor id_col in ['customerID','CustomerID','id','ID']:\n    if id_col in df.columns:\n        df.drop(columns=[id_col], inplace=True)\n        print(\"Dropped\", id_col)\nprint(\"Shape after drop:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparar variable target y revisar nulos"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\ntarget_candidates = ['Churn','churn','Evasion','Evasión','Cancelacion','Cancelación','target']\ntarget_col = next((c for c in df.columns if c in target_candidates), None)\nif target_col is None:\n    for c in df.columns:\n        if df[c].nunique()==2:\n            target_col = c; break\nif target_col is None:\n    raise ValueError(\"No se pudo detectar la columna target (Churn). Indica manualmente el nombre en target_col.\")\n\nprint(\"Target detected:\", target_col)\ndf[target_col] = df[target_col].astype(str).str.strip().str.lower().map({'no':0,'n':0,'false':0,'0':0,'si':1,'sí':1,'yes':1,'y':1,'true':1,'1':1}).astype(int)\nprint(df[target_col].value_counts())\nprint(\"\\nNulos por columna:\")\nprint(df.isna().sum().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One-hot encoding para categóricas (solo con pocas categorías)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\ncat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\ncat_cols = [c for c in cat_cols if c != target_col]\nprint(\"Categorical columns:\", cat_cols)\nohe_cols = [c for c in cat_cols if df[c].nunique() < 30]\nprint(\"Columns to OHE:\", ohe_cols)\ndf_ohe = pd.get_dummies(df, columns=ohe_cols, drop_first=True)\nprint(\"Shape after OHE:\", df_ohe.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chequeo de desbalance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\nimport matplotlib.pyplot as plt\ny = df_ohe[target_col]\nprint(\"Counts:\\n\", y.value_counts())\nprint(\"Proportions:\\n\", y.value_counts(normalize=True))\n\nplt.figure(figsize=(4,3))\nplt.bar(['No','Yes'], y.value_counts().values)\nplt.title('Distribución de Churn')\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Opcional: SMOTE para balancear clases"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\napply_smote = True\n\nif apply_smote:\n    try:\n        from imblearn.over_sampling import SMOTE\n        X = df_ohe.drop(columns=[target_col])\n        sm = SMOTE(random_state=42)\n        X_res, y_res = sm.fit_resample(X, df_ohe[target_col])\n        print('After SMOTE:', y_res.value_counts())\n    except Exception as e:\n        print('imblearn no disponible o error en SMOTE:', e)\n        X_res, y_res = df_ohe.drop(columns=[target_col]), df_ohe[target_col]\nelse:\n    X_res, y_res = df_ohe.drop(columns=[target_col]), df_ohe[target_col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Normalización para modelos sensibles (StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# Convert to DataFrame if needed\nX_res_df = X_res.copy()\nX_res_df = X_res_df.reset_index(drop=True)\nX_res_df.head()\nX_scaled_all = scaler.fit_transform(X_res_df)\nprint('Scaled shape:', X_scaled_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Matriz de correlación (numéricas) y correlación con churn"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\nimport numpy as np\nnum_df = pd.concat([X_res_df, y_res.reset_index(drop=True)], axis=1)\ncorr = num_df.corr()\nprint('Top correlations with target:')\nprint(corr[target_col].abs().sort_values(ascending=False).head(15))\nplt.figure(figsize=(10,8))\nplt.imshow(corr.values, aspect='auto')\nplt.colorbar()\nplt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\nplt.yticks(range(len(corr.index)), corr.index)\nplt.title('Matriz de correlación (numérica)')\nplt.tight_layout()\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Boxplots: Tenure and MonthlyCharges (si existen)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\ntenure_candidates = [c for c in X_res_df.columns if 'tenure' in c.lower() or 'time' in c.lower() or 'months' in c.lower()]\nmonthly_candidates = [c for c in X_res_df.columns if 'monthly' in c.lower() or 'charge' in c.lower() or 'costo' in c.lower() or 'factur' in c.lower()]\nprint('Tenure candidates', tenure_candidates)\nprint('Monthly candidates', monthly_candidates)\nimport matplotlib.pyplot as plt\nfor tc in tenure_candidates[:2]:\n    plt.figure(figsize=(6,4))\n    plt.boxplot([X_res_df.loc[y_res==0, tc], X_res_df.loc[y_res==1, tc]], labels=['No','Yes'])\n    plt.title(f\"{tc} vs Churn\")\n    plt.show()\nfor mc in monthly_candidates[:2]:\n    plt.figure(figsize=(6,4))\n    plt.boxplot([X_res_df.loc[y_res==0, mc], X_res_df.loc[y_res==1, mc]], labels=['No','Yes'])\n    plt.title(f\"{mc} vs Churn\")\n    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. División entrenamiento / prueba (70/30) y escalado para train/test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_res_df, y_res, test_size=0.3, random_state=42, stratify=y_res)\nscaler2 = StandardScaler().fit(X_train)\nX_train_scaled = scaler2.transform(X_train)\nX_test_scaled = scaler2.transform(X_test)\nprint('Train/Test shapes:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Modelado: Logistic Regression (scaled) y Random Forest (unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score\n\n# Logistic Regression (uses scaled data)\nlog = LogisticRegression(max_iter=1000, random_state=42)\nlog.fit(X_train_scaled, y_train)\ny_pred_log = log.predict(X_test_scaled)\ny_prob_log = log.predict_proba(X_test_scaled)[:,1]\nprint('Logistic Regression Report:')\nprint(classification_report(y_test, y_pred_log))\nprint('ROC AUC:', roc_auc_score(y_test, y_prob_log))\n\n# Random Forest (uses unscaled features)\nrf = RandomForestClassifier(n_estimators=200, random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\ny_prob_rf = rf.predict_proba(X_test)[:,1]\nprint('\\nRandom Forest Report:')\nprint(classification_report(y_test, y_pred_rf))\nprint('ROC AUC:', roc_auc_score(y_test, y_prob_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Importancia de variables y coeficientes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfeat_names = X_res_df.columns\nrf_imp = pd.Series(rf.feature_importances_, index=feat_names).sort_values(ascending=False)\nplt.figure(figsize=(10,5))\nrf_imp.head(15).plot(kind='bar')\nplt.title('RF - Top 15 feature importances')\nplt.show()\n\nlog_coef = pd.Series(log.coef_[0], index=feat_names).sort_values(ascending=False)\nplt.figure(figsize=(10,5))\nlog_coef.head(15).plot(kind='bar')\nplt.title('Logistic - Top 15 coefficients')\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Guardar modelos y preprocesado\nSe guardan `models/logistic_model.joblib`, `models/random_forest_model.joblib` y `data/TelecomX_preprocessed.csv` cuando se ejecuta."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\nimport joblib\nos.makedirs('models', exist_ok=True)\njoblib.dump(log, 'models/logistic_model.joblib')\njoblib.dump(rf, 'models/random_forest_model.joblib')\nprint('Models saved in models/ folder')\n\npd.DataFrame(X_res_df, columns=X_res_df.columns).assign(**{target_col: y_res.values}).to_csv('data/TelecomX_preprocessed.csv', index=False)\nprint('Saved data/TelecomX_preprocessed.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Informe — Conclusiones y Recomendaciones\nRellena este apartado con los insights obtenidos tras ejecutar el notebook: métricas por modelo, variables más importantes, y recomendaciones de negocio (segmentación, ofertas, mejora de métodos de pago, campañas de retención, etc.)."
   ]
  }
 ]
}